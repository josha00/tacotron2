{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_j1_qOQApDx"
      },
      "outputs": [],
      "source": [
        "#connect google drive\n",
        "%cd /content/\n",
        "from google.colab import drive\n",
        "drive.mount('drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure you are connected to a GPU runtime"
      ],
      "metadata": {
        "id": "Z6V-3dHn1uQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8L4MWW2As-G"
      },
      "outputs": [],
      "source": [
        "#get tacotron2 from github\n",
        "%cd /content\n",
        "!git clone 'https://github.com/josha00/tacotron2.git' tacotron2\n",
        "%cd /content/tacotron2\n",
        "!git checkout josha00-patch-1\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "\n",
        "#Install requirements\n",
        "!pip install -r requirements.txt\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(0)\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get pretrained tacotron model to use as starting point (note: this one is the LJ pretrained model)\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA', '/content/tacotron2/pretrained_model', quiet=False)\n"
      ],
      "metadata": {
        "id": "uD-kSsRHg1lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQXO2Fy7CB6V"
      },
      "outputs": [],
      "source": [
        "#upload audio files for training\n",
        "\n",
        "os.chdir('/content/tacotron2')\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "if os.path.exists('/content/tacotron2/wavs'):\n",
        "  shutil.rmtree('wavs')\n",
        "\n",
        "audio_path = \"/content/drive/MyDrive/_/wavs/.\"\n",
        "audio_to = \"/content/tacotron2/wavs\"\n",
        "shutil.copytree(audio_path, audio_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn2FOsqx9npi"
      },
      "outputs": [],
      "source": [
        "  #upload ONLY SOME audio files for training\n",
        "os.chdir('/content/tacotron2')\n",
        "\n",
        "upload_some=True #set to true if want this\n",
        "\n",
        "percent=.5     #percentage of the audio files you want to upload\n",
        "\n",
        "if upload_some:\n",
        "    !pip install tqdm\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    audio_path = \"/content/drive/MyDrive/_/wavs/\"\n",
        "    audio_to = \"/content/tacotron2/wavs\"\n",
        "    audio_files = os.listdir(audio_path)\n",
        "    audio_files.sort()\n",
        "\n",
        "    # Select some files only\n",
        "    selected_files=audio_files\n",
        "\n",
        "    split_point=round(len(audio_files)*percent)  #amount of audio files you want to upload\n",
        "\n",
        "    random.shuffle(selected_files)\n",
        "    selected_files=selected_files[:split_point]\n",
        "    selected_files.sort()\n",
        "\n",
        "    os.makedirs(audio_to, exist_ok=True)\n",
        "\n",
        "    for file_name in tqdm(selected_files, desc=\"Copying Files\", unit=\"file\"):\n",
        "        source_file_path = os.path.join(audio_path, file_name)\n",
        "        dest_file_path = os.path.join(audio_to, file_name)\n",
        "        shutil.copy2(source_file_path, dest_file_path)\n",
        "\n",
        "    print(\"Uploaded audio files to\", audio_to)\n",
        "    print('audio files:',len(os.listdir('wavs')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO3QN_gWaTNu"
      },
      "outputs": [],
      "source": [
        "#change audio bits/sample and sample rate if needed\n",
        "\n",
        "change=False     #set to true if needed\n",
        "\n",
        "def convert_audio(input_file_path, output_file_path):\n",
        "    !sox -v 0.95 \"$input_file_path\" -r 22050 -b 16 \"$output_file_path\" #reduce 0.95 further if get clipping issues\n",
        "\n",
        "def process_audio_files(input_dir, output_dir):\n",
        "\n",
        "    audio_files = os.listdir(input_dir)\n",
        "    audio_files.sort()\n",
        "    selected_files=audio_files\n",
        "    selected_files.sort()\n",
        "    amo=len(selected_files)\n",
        "\n",
        "    for index, file_name in enumerate(selected_files):\n",
        "        input_file_path = os.path.join(input_dir, file_name)\n",
        "        output_file_path = os.path.join(output_dir, file_name)\n",
        "        convert_audio(input_file_path, output_file_path)\n",
        "        print(f\"file {index} of {amo} format changed\")\n",
        "\n",
        "input_directory = '/content/tacotron2/wavs'\n",
        "output_directory = '/content/tacotron2/wavs'\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "if change:\n",
        "    process_audio_files(input_directory, output_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKYQZZYMaXyw"
      },
      "outputs": [],
      "source": [
        "#TRIM OUT SILENCE\n",
        "\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_leading_silence, detect_leading_silence\n",
        "\n",
        "def trim_start_and_end(audio_path, output_path, silence_threshold,silence_threshold2):\n",
        "\n",
        "    audio = AudioSegment.from_file(audio_path, format=\"wav\")\n",
        "\n",
        "    # Find the start and end of non-silent sections\n",
        "    start_trim = detect_leading_silence(audio, silence_threshold)\n",
        "    end_trim = detect_leading_silence(audio.reverse(), silence_threshold2)\n",
        "\n",
        "    # Calculate the new audio with trimmed start and end\n",
        "    trimmed_audio = audio[start_trim:len(audio)-end_trim]\n",
        "\n",
        "    # Export the trimmed audio to the specified output path\n",
        "    trimmed_audio.export(output_path, format=\"wav\")\n",
        "\n",
        "input_folder = 'wavs'\n",
        "output_folder = 'wavs'\n",
        "\n",
        "# Call the trimming function for each file\n",
        "for filename in os.listdir(input_folder):\n",
        "  if filename.endswith(\".wav\"):\n",
        "        input_audio_path = os.path.join(input_folder, filename)\n",
        "        output_audio_path = os.path.join(output_folder, filename)\n",
        "        trim_start_and_end(input_audio_path, output_audio_path, -40,-50)    #increasing (closer to 0) cuts more audio\n",
        "\n",
        "print(\"Trimming complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1o0EcibxT2l"
      },
      "outputs": [],
      "source": [
        "#get audio duration\n",
        "import librosa\n",
        "\n",
        "folder_path = \"wavs\"\n",
        "total_duration = 0\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        audio_file = os.path.join(folder_path, filename)\n",
        "        y, sr = librosa.load(audio_file)\n",
        "        duration = librosa.get_duration(y=y, sr=sr)\n",
        "        total_duration += duration\n",
        "\n",
        "print(f\"Total Duration of Audio Clips: {total_duration/60} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert audio to mels (code modified from data_utils.py)\n",
        "\n",
        "##################################################\n",
        "\n",
        "meldirect=False   #set to true to get mels from files directly in colab, rather than copying files to colab first\n",
        "wav_location = '/content/drive/MyDrive/wavs'\n",
        "\n",
        "##################################################\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "hparams = create_hparams()\n",
        "\n",
        "stft = layers.TacotronSTFT(\n",
        "            hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "            hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "            hparams.mel_fmax)\n",
        "\n",
        "def get_mel(filename):\n",
        "  audio, sampling_rate = load_wav_to_torch(filename)\n",
        "  if sampling_rate != stft.sampling_rate:\n",
        "    raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "  sampling_rate, stft.sampling_rate))\n",
        "  audio_norm = audio / hparams.max_wav_value\n",
        "  audio_norm = audio_norm.unsqueeze(0)\n",
        "  audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "  melspec = stft.mel_spectrogram(audio_norm)\n",
        "  melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "  return melspec\n",
        "\n",
        "if meldirect:\n",
        "  wav_dir=wav_location\n",
        "  print(wav_location)\n",
        "else:\n",
        "  wav_dir='/content/tacotron2/wavs'\n",
        "\n",
        "if not os.path.exists('/content/tacotron2/wavs'):\n",
        "        os.makedirs('/content/tacotron2/wavs')\n",
        "\n",
        "wav_files = [f for f in os.listdir(wav_dir) if f.endswith('.wav')]\n",
        "for wf in wav_files:\n",
        "  input_path = os.path.join(wav_dir, wf)\n",
        "  mel_spectogram = get_mel(input_path)\n",
        "  output_path = os.path.join('/content/tacotron2/wavs', os.path.splitext(wf)[0] + '.npy')\n",
        "  np.save(output_path, mel_spectogram)\n",
        "\n",
        "os.chdir('/content/tacotron2')"
      ],
      "metadata": {
        "id": "G8Hr3dkpP6S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKNbjb-5CSkj"
      },
      "outputs": [],
      "source": [
        "#upload transcript (in format name.npy|transcription)\n",
        "\n",
        "%cd /content/tacotron2/filelists/\n",
        "if os.path.exists('list.txt'):\n",
        "  !rm list.txt\n",
        "\n",
        "transcript_path = \"/content/drive/MyDrive/_/list.txt\"\n",
        "copy_to = \"/content/tacotron2/filelists\"\n",
        "\n",
        "shutil.copy(transcript_path, os.path.join(copy_to, 'list.txt'))\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwmj_Xto32gW"
      },
      "outputs": [],
      "source": [
        "#delete any file lines that have no matching audio\n",
        "\n",
        "def delete_lines(transcript):\n",
        "  with open(transcript, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  adjusted_lines = []\n",
        "  for line in lines:\n",
        "      path_to_audio = '/content/tacotron2/'+ line.split('|')[0][:-4] + '.wav'\n",
        "\n",
        "      if os.path.exists(path_to_audio):\n",
        "          adjusted_lines.append(line)\n",
        "      if not os.path.exists(path_to_audio):\n",
        "          print(path_to_audio)\n",
        "\n",
        "  with open(transcript, \"w\") as f:\n",
        "    f.writelines(adjusted_lines)\n",
        "\n",
        "delete_lines(\"/content/tacotron2/filelists/list.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35U_ky-aMdDF"
      },
      "outputs": [],
      "source": [
        "#split files into train and validate\n",
        "\n",
        "train_amount=.5      #set what percentage of the dataset you want for training\n",
        "\n",
        "with open('/content/tacotron2/filelists/list.txt', 'r') as list_file:\n",
        "  wav_files = [line.strip() for line in list_file]\n",
        "\n",
        "random.shuffle(wav_files)\n",
        "train_to=round(len(wav_files)*train_amount)\n",
        "\n",
        "\n",
        "# Create train_files.txt\n",
        "train_files = wav_files[:train_to]\n",
        "train_files.sort()\n",
        "with open(os.path.join('filelists', 'train_files.txt'), 'w') as f_train:\n",
        "    for filename in train_files:\n",
        "        f_train.write(os.path.join(filename) + '\\n')\n",
        "\n",
        "# Create val_files.txt\n",
        "val_files = wav_files[train_to:]\n",
        "val_files.sort()\n",
        "with open(os.path.join('filelists', 'val_files.txt'), 'w') as f_val:\n",
        "    for filename in val_files:\n",
        "        f_val.write(os.path.join(filename) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get number of training clips\n",
        "\n",
        "with open('filelists/train_files.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "num_train = len(lines)\n",
        "print(f\"Number of training clips: {num_train}\")\n"
      ],
      "metadata": {
        "id": "gMlI0Hd4l5ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get training audio duration\n",
        "import librosa\n",
        "\n",
        "total_duration = 0\n",
        "\n",
        "with open('/content/tacotron2/filelists/train_files.txt', 'r') as file:\n",
        "  train_files_content = file.read()\n",
        "\n",
        "for filename in os.listdir(\"wavs\"):\n",
        "  if filename.endswith(\".wav\"):\n",
        "      if filename[:-4] in train_files_content:\n",
        "        audio_file = os.path.join('/content/tacotron2/wavs', filename)\n",
        "        y, sr = librosa.load(audio_file)\n",
        "        duration = librosa.get_duration(y=y, sr=sr)\n",
        "        total_duration += duration\n",
        "\n",
        "print(f\"Total Duration of Training Audio Clips: {total_duration/60} minutes\")"
      ],
      "metadata": {
        "id": "EATgdSsvKFYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykZX4AgaXXuf"
      },
      "outputs": [],
      "source": [
        "#edited version of train.py code\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    print(\"Saving model at iteration {} to {}\".format(\n",
        "        iteration, filepath))\n",
        "    torch.save({'iteration': iteration,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'learning_rate': learning_rate}, filepath)\n",
        "\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate,train_loss):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        counti=0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            counti+=1\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (counti)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "      print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "      logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "\n",
        "\n",
        "\n",
        "      %matplotlib inline\n",
        "      _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "      idx = random.randint(0, alignments.size(0) - 1)\n",
        "      alignment=(alignments[idx].data.cpu().numpy().T)\n",
        "      fig, ax = plt.subplots(figsize=(5, 3))\n",
        "      im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',interpolation='none')\n",
        "      plt.tight_layout()\n",
        "      fig.canvas.draw()\n",
        "      plt.show()\n",
        "\n",
        "      #save to outputs to another folder\n",
        "      import csv\n",
        "      csv_file_path = os.path.join(output_directory,'val_loss.csv')\n",
        "      loss_entry = [epoch, val_loss,train_loss,iteration,(time.perf_counter()-start_eposh)/60,learning_rate]\n",
        "      with open(csv_file_path, 'a', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        if os.path.getsize(csv_file_path) == 0:\n",
        "          csv_writer.writerow([\"Epoch\", \"Validation Loss\", \"Training Loss\",\"Iteration\", \"Time\",\"Learning Rate\"])\n",
        "        csv_writer.writerow(loss_entry)\n",
        "        csv_file.flush()\n",
        "\n",
        "      alignments_dir = os.path.join(output_directory, 'alignments')\n",
        "      os.makedirs(alignments_dir, exist_ok=True)\n",
        "      file_name = f'alignment_epoch_{epoch}.png'\n",
        "      image_path = os.path.join(alignments_dir, file_name)\n",
        "      fig.savefig(image_path)\n",
        "\n",
        "\n",
        "        #print(\"Validation loss {}: {:9f}  \".format(iteration, val_loss))    original code\n",
        "        #logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, save_interval):\n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    epoch_offset = 0\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    else:\n",
        "      if warm_start:\n",
        "        model = warm_start_model(\"/content/tacotron2/pretrained_model\", model, hparams.ignore_layers)\n",
        "\n",
        "    start_eposh = time.perf_counter()\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "\n",
        "    if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "    if not os.path.isdir(log_directory):\n",
        "            os.makedirs(log_directory)\n",
        "\n",
        "\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in range(epoch_offset, hparams.epochs):\n",
        "        train_loss = 0\n",
        "        icount=0\n",
        "        print(\"Epoch: {}\".format(epoch))\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            icount+=1\n",
        "            start = time.perf_counter()\n",
        "            learning_rate=getLR(epoch)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            model.zero_grad()\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            if hparams.distributed_run:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "            if hparams.fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if hparams.fp16_run:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                is_overflow = math.isnan(grad_norm)\n",
        "            else:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if not is_overflow and rank == 0:\n",
        "                duration = time.perf_counter() - start\n",
        "                logger.log_training(\n",
        "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "            train_loss += reduced_loss\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        train_loss = train_loss/(icount)\n",
        "\n",
        "        validate(model, criterion, valset, iteration,\n",
        "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate,train_loss)\n",
        "\n",
        "\n",
        "        if (epoch+1) % save_interval == 0 or (epoch+1) == hparams.epochs:\n",
        "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path + 'epoch' + str(epoch+1))\n",
        "\n",
        "hparams = create_hparams()\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIL9IsRfAFVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UPgyAypOCj5"
      },
      "outputs": [],
      "source": [
        "#set parameters\n",
        "\n",
        "hparams = create_hparams()\n",
        "hparams.training_files =\"filelists/train_files.txt\"\n",
        "hparams.validation_files = \"filelists/val_files.txt\"\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "################################################################\n",
        "def getLR(epoch):\n",
        "      if epoch<=50:\n",
        "        LR=5e-5\n",
        "      elif epoch>50 and epoch<=100:\n",
        "        LR=2e-5\n",
        "      else:\n",
        "        LR=1e-5\n",
        "      return LR\n",
        "\n",
        "model_name='NAME'    #set name of model\n",
        "warm_start=True   #if true, train from pretrained model starting point\n",
        "save_interval = 50 #how often you want to save a copy of the model\n",
        "hparams.epochs = 200\n",
        "hparams.batch_size = min(num_train,1)\n",
        "################################################################\n",
        "\n",
        "\n",
        "output_directory = os.path.join('/content/drive/MyDrive/colab/', model_name)\n",
        "log_directory = os.path.join(output_directory, 'Logs')\n",
        "checkpoint_path = output_directory+(r'/')+'model'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: if you want to continue training from a saved checkpoint, eg from epoch 50:\n",
        "* in your output directory you will have a model file called 'NAMEepoch50'.\n",
        "* rename this to 'model' and your training will now start from this checkpoint.\n"
      ],
      "metadata": {
        "id": "80NQppbk1T2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN TRAINING\n",
        "\n",
        "train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus, rank, group_name, hparams,save_interval)"
      ],
      "metadata": {
        "id": "CGtaTart1myc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zEiiJxXq9L_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
